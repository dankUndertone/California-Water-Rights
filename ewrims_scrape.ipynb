{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the EWRIMS Database\n",
    "\n",
    "A walk through for scraping the entire EWRIMS database. This module allows for appending new applications and reports to existing datasets. This module was developed in consultation with the Center for Environmental Economics and Sustainability Policy at the W.P. Carey School of Business under the direction of Dr. William Michael Hanemann.\n",
    "\n",
    "* **Applications:** each application is an application to the water rights for a specific body of water. Applications may detail beneficial water use among other pertinant details. Applications may be submitted multiple times over time as required by local authorities, private contracts, or others. The state of California does not penalize a failure to apply for water rights so this database is not a complete representation of water use in the state. <br><br>\n",
    "\n",
    "* **Reports:** reports may be submitted in association with particular applications. Reports cover a wide vireity of topics and may describe anything from the amount of water used over time to conservations efforts to changes to the project to methods of measurement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from datetime import datetime\n",
    "\n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import bear_necessities as bn \n",
    "import ewrmis_master_data as ewd\n",
    "\n",
    "from importlib import reload\n",
    "ewd = reload(ewd)\n",
    "\n",
    "from ewrmis_master_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Scrape the all the applications currently hosted in the database*\n",
    "599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: chrome not reachable\n",
      "  (Session info: chrome=75.0.3770.142)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "update_complete = False\n",
    "\n",
    "while update_complete==False: \n",
    "    try: \n",
    "        application_data = ewd.scrape_applications()\n",
    "        update_complete = True\n",
    "        print('Update Completed')\n",
    "    except Exception as e: \n",
    "        print(str(e))\n",
    "        # if we don't error out due to time \n",
    "        if 'timeout' not in str(e): \n",
    "            update_complete=True \n",
    "        else: \n",
    "        # if we timed out take a break for 2 seconds and retry\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58169"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "application_data = bn.loosen(os.getcwd()+'/data/database/database/master_data.pickle')\n",
    "# drop duplicate applications\n",
    "application_data = application_data.drop_duplicates(subset='ApplID')\n",
    "# remove spaces from the column names \n",
    "application_data.columns = [c.replace(' ','') for c in application_data.columns]\n",
    "\n",
    "# format the dates in the scraped application data into datetime format \n",
    "application_data.loc[application_data['Date']!='','Date'] = application_data.loc[application_data['Date']!='','Date'].apply(lambda s:datetime.strptime(s, '%m/%d/%Y'))\n",
    "len(application_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load an existing dataset with application data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55287"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "application_master = pd.read_csv(os.getcwd()+'/data/database/database/master_data.csv')\n",
    "# format the dates in the older application data into datetime format \n",
    "application_master.loc[application_master['date'].notnull(),'date']= application_master.loc[application_master['date'].notnull(),'date'].apply(lambda s:datetime.strptime(s, '%m/%d/%Y'))\n",
    "len(application_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recent date from newest scrape:  2019-07-18 00:00:00\n",
      "Most recent date from oldest scrape:  2018-09-20 00:00:00\n"
     ]
    }
   ],
   "source": [
    "new_max = application_data.loc[application_data['Date']!='','Date'].max()\n",
    "old_max = application_master.loc[application_master['date'].notnull(),'date'].max()\n",
    "print(\"Most recent date from newest scrape: \", new_max)\n",
    "print(\"Most recent date from oldest scrape: \", old_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check how many applications have been submitted (new) or modifieid since the last scrape*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of new or modified applications since last date:  1780\n"
     ]
    }
   ],
   "source": [
    "new_or_modified = application_data.loc[application_data['Date']!=''].loc[application_data.loc[application_data['Date']!='','Date']>old_max]\n",
    "print(\"# of new or modified applications since last date: \", str(len(new_or_modified)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check how many new application IDs have been submitted*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Applications:  1537\n",
      "New Applications Records:  2119\n"
     ]
    }
   ],
   "source": [
    "new_applications = np.setdiff1d(application_data['ApplID'].values, application_master['ApplID'].values)\n",
    "print('New Applications: ',str(len(new_applications)))\n",
    "\n",
    "new_applications = application_data.set_index('ApplID').loc[new_applications].reset_index()\n",
    "print('New Applications Records: ',str(len(new_applications)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1966\n",
      "1964\n"
     ]
    }
   ],
   "source": [
    "# Check how many unique new or modified applications. \n",
    "apps = new_applications.append(new_or_modified, ignore_index=True, sort=False)\n",
    "print(len(apps.drop_duplicates()))\n",
    "print(len(apps.drop_duplicates(subset='ApplID')))\n",
    "\n",
    "# We will scrape the application and reports for unique new or modified applications \n",
    "apps = apps.drop_duplicates(subset='ApplID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This function automatically creates a \".pickle\" file in the database for each of these tables for each 100 scraped and upon finishing. It keeps track of progress using the `current_application.txt` file in the runtime folder. Set the value in the .txt file to 0 to restart the scraping from the 0th row in the submitted application data.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the for every application\n",
    "summary_data, current_parties, histparties, record_summary, sources, uses, report, decisions = scrape_application_details(apps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can load the report links so we can scrape the details from each report*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = bn.loosen(os.getcwd()+'/data/database/database/electronic_reports.pickle')\n",
    "reports['ViewReportPDF'] = reports['ViewReportPDF'].apply(lambda x: x.replace('..','https://ciwqs.waterboards.ca.gov/ciwqs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here we scrape the links from each report*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links = list(reports['ViewReportPDF'].values)\n",
    "ids = list(reports['ApplID'].values)\n",
    "datasets = scrape_reports(links, ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
